<article>
  <h2>Ai is trash unless locally</h2>
  <p><em>Jun 07, 2025</em></p>
  <p>
    Running AI models locally doesn’t have to be complicated. With
    <a href="https://ollama.com/">Ollama</a>
    , it’s surprisingly simple and powerful.
  </p>
  <p>
    Ollama is a lightweight tool that lets you run open-source large language models directly on your machine. Whether
    you're experimenting, building apps, or exploring AI, it supports models like DeepSeek-R1, Qwen 3, Llama 3.3, Qwen
    2.5‑VL, Gemma 3, and many others.
  </p>
  <h2>Why Run Models Locally?</h2>
  <p>Local deployment gives you privacy, control, and offline access Ollama makes this easier than ever.</p>
  <h2>Installation Guide</h2>
  <h3>Windows</h3>
  <ol>
    <li>
      Download the installer from
      <a href="https://ollama.com/download">ollama.com/download</a>
    </li>
    <li>Run the `.exe` file and follow the prompts.</li>
    <li>
      Open Command Prompt or PowerShell and run:
      <pre><code>ollama run llama3</code></pre>
    </li>
  </ol>
  <h3>macOS</h3>
  <ol>
    <li>
      Open Terminal and run:
      <pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>
    </li>
    <li>
      Then start a model:
      <pre><code>ollama run llama3</code></pre>
    </li>
  </ol>
  <h3>Linux</h3>
  <ol>
    <li>
      Install using the command:
      <pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>
    </li>
    <li>
      Run a model:
      <pre><code>ollama run llama3</code></pre>
    </li>
  </ol>
  <h3>Docker</h3>
  <ol>
    <li>Make sure Docker is installed and running.</li>
    <li>
      Pull and run the Ollama image:
      <pre><code>docker run -d -p 11434:11434 ollama/ollama</code></pre>
    </li>
    <li>
      Then, from your host machine, you can run:
      <pre><code>ollama run llama3</code></pre>
    </li>
  </ol>
  <h2>Final Thoughts</h2>
  <p>
    Ollama is a beautiful tool for anyone looking to explore or integrate local AI capabilities. It’s free, open, and
    supports a growing list of models. Give it a try you might be surprised how easy and fast it is to get started.
  </p>
</article>
